<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="keywords" content="create a sequential dump of huge tables, baan,baanerp,erp,forum,discussion,bulletin board" />
	<meta name="description" content="[Archive] create a sequential dump of huge tables Tools Administration &amp; Installation" />
	
	<title>create a sequential dump of huge tables [Archive]  - Baanboard.com</title>
	<link rel="stylesheet" type="text/css" href="../styles.css" />
</head>
<body>
<div class="pagebody">
<div id="navbar"><a href="../index.html">Newbaanboard</a> &gt; <a href="../index.html">Baan Quick Support: Functional &amp; Technical</a> &gt; <a href="../index.html">Tools Administration &amp; Installation</a> &gt; create a sequential dump of huge tables</div>
<hr />
<div class="pda"></div>

<hr />

<div class="post"><div class="posttop"><div class="username">mig28mx</div><div class="date">18th November 2003, 01:38</div></div><div class="posttext">I´m using baan Vc2 mcr. The tables are huge so I can´t perform a simple secuential dump for all tables. So I create many of the secuential tables dump in order to cover all of them.<br />
My question is.- Is there any tool to perform this automatically?</div></div><hr />


<div class="post"><div class="posttop"><div class="username">Gerhard</div><div class="date">18th November 2003, 08:35</div></div><div class="posttext">Hi mig28mx,<br />
<br />
yes there is such a tool: <br />
bdbpre6.1 :D <br />
<br />
If your portingset isn't too old, you can use the option -M to define the maximum size of a dumpfile.<br />
You can see the possible options and the syntax with bdbpre6.1 -U.</div></div><hr />


<div class="post"><div class="posttop"><div class="username">mig28mx</div><div class="date">18th November 2003, 17:48</div></div><div class="posttext">Hi Gerhard,<br />
Thanks for your reply. Yes I can use bdbpre6.1 but, what happen when the file reach the maximum allowed? In Unix the limit are 2gb, right?</div></div><hr />


<div class="post"><div class="posttop"><div class="username">fosterjr</div><div class="date">18th November 2003, 18:30</div></div><div class="posttext">We used to have the same problem.<br />
<br />
We would use bdbpre6.1 in a script.  The script would create a pipe, bdbpre6.1 would output to the pipe and then the script would cat the pipe to compress to the final output file.<br />
<br />
If the file winds up being still too large, you can use bdbpre6.1 with a the -I option.  -I is the input file.  In the input file, list all the tables you want to dump.  Set up multiple input files and run the script for each of the input files to create smaller dumps.<br />
<br />
<br />
<br />
/etc/mknod $tmpdmp p<br />
cat $tmpdmp | compress &gt; $thedmp &amp;<br />
<br />
bdbpre6.1 -o $tmpdmp  -Itables.txt</div></div><hr />



</div>
</body>
</html>