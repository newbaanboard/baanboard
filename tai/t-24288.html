<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="keywords" content="Other than ttaad4226m000, baan,baanerp,erp,forum,discussion,bulletin board" />
	<meta name="description" content="[Archive] Other than ttaad4226m000 Tools Administration &amp; Installation" />
	
	<title>Other than ttaad4226m000 [Archive]  - Baanboard.com</title>
	<link rel="stylesheet" type="text/css" href="../styles.css" />
</head>
<body>
<div class="pagebody">
<div id="navbar"><a href="../index.html">Newbaanboard</a> &gt; <a href="../index.html">Baan Quick Support: Functional &amp; Technical</a> &gt; <a href="../index.html">Tools Administration &amp; Installation</a> &gt; Other than ttaad4226m000</div>
<hr />
<div class="pda"></div>

<hr />

<div class="post"><div class="posttop"><div class="username">macahu</div><div class="date">14th June 2005, 19:00</div></div><div class="posttext">Hi all,<br />
<br />
Can someone tell me if there is another way to get a sequentiel dump of table  of our live company. Because ttaad4226m000 takes around 5 hours and we execute it daily.<br />
<br />
Thank you for your help</div></div><hr />


<div class="post"><div class="posttop"><div class="username">norwim</div><div class="date">14th June 2005, 21:02</div></div><div class="posttext">Hi there,<br />
<br />
was this session basically does is to call bdbpre6.1 (or bdbpre6.2 in case of Baan V).<br />
bdbpre6.1 -U shows usage, what really helps is the -f parameter, this suppresses usage of the index and reads data only. (hope it really is '-f', no system at hand atm to check)<br />
What also might be very handy is to paralize execution (as a thumbrule: start 1 bdbpre6.1 less than you have CPUs).<br />
So: create a file with the tablenames to be dumped.<br />
Divide this file into several files (taking care that each file contains more or less the same number of names of large tables)<br />
Start several bdbpre6.1 processes with one file each.<br />
<br />
bdbpre6.1 -f -Ifilename1 -pPACKAGE_COMB -CCOMPNO &gt; part1<br />
bdbpre6.1 -f -Ifilename2 -pPACKAGE_COMB -CCOMPNO &gt; part2 .. etc<br />
<br />
(From memory ... check syntax with bdbpre6.1 -u/U)<br />
<br />
That should cut down the time drastically.<br />
<br />
hth<br />
<br />
Norbert</div></div><hr />


<div class="post"><div class="posttop"><div class="username">NPRao</div><div class="date">14th June 2005, 21:12</div></div><div class="posttext">Norbert's idea works if you can setup multiple jobs/shell-scripts to work on different companies in parallel. <br />
<br />
You can also change the db_resource settings to change the values of - ssts_set_rows, ora_max_array_fetch, ora_max_array_insert to increase the speed.<br />
<br />
Also be aware that this dump process locks the table, make sure there are no users/active jobs in the system at that time.<br />
<br />
Why dont you consider taking the Oracle hot/cold-backups ?</div></div><hr />


<div class="post"><div class="posttop"><div class="username">norwim</div><div class="date">15th June 2005, 02:16</div></div><div class="posttext">agreed NPRao,<br />
<br />
I think that ora_max_array_fetch is the parameter that speeds up dumps if increased, but why do you say 'different companies'?<br />
The main idea is to gain speed by having more than one dump running at the same time.<br />
I was in fact thinking of one company.<br />
Of course this assumes that noone is working in this company from the start of the first dump until the completion of the last ... but to create a consistent dump this would have to have been the case anyway for the 5 hours that one dump took without parallelizing.<br />
Only that (depending on the number of bdbpre-jobs running parallelly) the downtime would be  significantly shorter.<br />
<br />
regards<br />
<br />
Norbert<br />
<br />
P.S.: What is the correct english diction for 'having jobs run in parallel' btw?  'paralize' sounds very wrong now that I read my former post :-(</div></div><hr />


<div class="post"><div class="posttop"><div class="username">NPRao</div><div class="date">15th June 2005, 03:43</div></div><div class="posttext">but why do you say 'different companies'?<br />
Sorry I wasn't clear. I meant he can divide the dump process, each script for a particular company. For a single company, one can generate dumps by the table lists or by package list to make it faster. Either way we cannot guarantee the data consistency.<br />
<br />
I would suggest to use the Oracle cold-backups.</div></div><hr />


<div class="post"><div class="posttop"><div class="username">dave_23</div><div class="date">15th June 2005, 04:00</div></div><div class="posttext">If we're talking oracle there is no faster way than &quot;exp direct=y ...&quot;<br />
we're talking lightning fast.<br />
<br />
In SQL Server you could sp_detatch, copy file, done.<br />
<br />
i'm sure there's a way to do something similar with informix and db2 as<br />
well.<br />
<br />
Dave</div></div><hr />


<div class="post"><div class="posttop"><div class="username">victor_cleto</div><div class="date">15th June 2005, 18:29</div></div><div class="posttext">You should do backups from the database itself, I do not see any reasons to do daily exports unless strictly needed (except to create a daily copy on a test server running with a different database).<br />
If there is no other way except thru exports, then use the paralell bdbpre as mentioned before by splitting the table list.<br />
<br />
If your scripting capabilities are up to it (windows?), then I would generate those list(s) of tables from the database itself by selecting only the company tables that have a number of rows &gt; 0 to avoid Baan to dump empty tables. This saves time since a lot of tables are not used and thus are empty.</div></div><hr />


<div class="post"><div class="posttop"><div class="username">Brendan Shine</div><div class="date">15th June 2005, 21:55</div></div><div class="posttext">You may also save time by temporarily removing logical tables links (i.e., save off current $BSE/lib/compnr6.x file &amp; creating an empty one, doing bdbpre, putting links back).</div></div><hr />


<div class="post"><div class="posttop"><div class="username">victor_cleto</div><div class="date">16th June 2005, 11:24</div></div><div class="posttext">You may also save time by temporarily removing logical tables links (i.e., save off current $BSE/lib/compnr6.x file &amp; creating an empty one, doing bdbpre, putting links back).<br />
In fact this is not needed if you generate the list of tables (with rows &gt; 0) from the database to be used in the export, since the database does not know that there are tables linked to another company.<br />
<br />
Just be be aware that if you plan to build a standalone company from a multy-fincancial setup, these exports will not allow you to do so since the &quot;linked&quot; tables were not dumped at all.</div></div><hr />



</div>
</body>
</html>