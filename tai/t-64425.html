<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="keywords" content="table dump question, baan,baanerp,erp,forum,discussion,bulletin board" />
	<meta name="description" content="[Archive] table dump question Tools Administration &amp; Installation" />
	
	<title>table dump question [Archive]  - Baanboard.com</title>
	<link rel="stylesheet" type="text/css" href="../styles.css" />
</head>
<body>
<div class="pagebody">
<div id="navbar"><a href="../index.html">Newbaanboard</a> &gt; <a href="../index.html">Baan Quick Support: Functional &amp; Technical</a> &gt; <a href="../index.html">Tools Administration &amp; Installation</a> &gt; table dump question</div>
<hr />
<div class="pda"></div>

<hr />

<div class="post"><div class="posttop"><div class="username">Eddie Monster</div><div class="date">2nd August 2013, 20:19</div></div><div class="posttext">I have to dump all the tables from one of our Baan companies.  I want to use bdbpre but I have a theory question on how to proceed. <br />
<br />
As Baanboardians, in your experience would you recommend that I use one command to dump all the tables one after another, or utilize multiple scripts to execute say 100 bdbpre commands in succession?<br />
<br />
Just looking for an efficient way to dump the tables in a controlled manner and be able to monitor for any errors that might occur.<br />
<br />
Any thoughts or experiences you could share would be greatly appreciated!<br />
<br />
Thank you in advance for any advice.</div></div><hr />


<div class="post"><div class="posttop"><div class="username">bhushanchanda</div><div class="date">2nd August 2013, 21:23</div></div><div class="posttext">Hi Eddie.<br />
<br />
Just a question, why do you want to go with command line if you have a Create Sequential Dump session already pressent. I don't think the execution times can vary a lot.<br />
<br />
But, for your reference you can go through these threads:-<br />
<br />
Thread 1 (http://www.baanboard.com/baanboard/showthread.php?t=5560)<br />
<br />
<br />
Thread 2 (http://www.baanboard.com/baanboard/showthread.php?t=26553)<br />
<br />
Thread 3 (http://www.baanboard.com/baanboard/showthread.php?t=1885)</div></div><hr />


<div class="post"><div class="posttop"><div class="username">pramod</div><div class="date">2nd August 2013, 21:37</div></div><div class="posttext">If you dump tables using the session ttaad4226m000, you have the range to choose the company and tables you want want to dump. This runs bdbpre in the background. <br />
<br />
See how I use it in command line.<br />
<br />
bdbpre -Eerr -I &lt;tables&gt; -o &lt;path_to_store_seq_files&gt; -t&quot;|&quot; -C &lt;company_number&gt;<br />
<br />
-I tables: -I is the option to read the input file; tables is the input file name; a text file with list of table you want to dump.<br />
<br />
Hope this helps you.<br />
Pramod</div></div><hr />


<div class="post"><div class="posttop"><div class="username">GaryEd</div><div class="date">2nd August 2013, 23:28</div></div><div class="posttext">We did this when moving to a new server and upgrading from sp18 to sp30 in the process.<br />
I ran 4 simultaneous bdbpre's from the command line each dumping tables listed in a file. I had previously used a report of rows by table to arrange the table lists so they would take about an equal amount of time by splitting the 4 biggest tables across the files followed by the next biggest 4 and so on. It worked very well and I used the same table lists to re-import the data on the new server.</div></div><hr />


<div class="post"><div class="posttop"><div class="username">Eddie Monster</div><div class="date">6th August 2013, 16:48</div></div><div class="posttext">Thank you for all the replies.  It was valuable information.</div></div><hr />


<div class="post"><div class="posttop"><div class="username">Brendan Shine</div><div class="date">8th August 2013, 01:32</div></div><div class="posttext">I concur with Gary's approach of splitting if time is of the essence otherwise standard session is simpler.  I have done both depending upon the business/technical requirements.  I've applied similar logic to bdbreconfiguration as well in addition to bdbpre.  However in latest LN porting set there is now an option for multiple bdbeconfig processes in parallel.</div></div><hr />


<div class="post"><div class="posttop"><div class="username">shah_bs</div><div class="date">8th August 2013, 20:31</div></div><div class="posttext">We use the session Create Sequential Dump of Tables.<br />
<br />
We have set the download up as a number of different jobs - for our purpose, one job per package. Inside the job, the session is run over and over for a differing range of tables.<br />
<br />
You may want to first do a 'bulk' analysis to find out how to 'spread' the tables in the sequential dump session in terms of megabytes. This does not have to be very rigourous - just approximate enough to somewhat even out the megabytes - sometimes it is impossible - for example, tdilc401 or tdinv700 are by themselves.<br />
<br />
To give a concept: for the td package, we found that the following distribution of the tables among the download session in the job approximately evens out the download:<br />
<br />
      table.f:=<br />
      table.t:=ilc300<br />
<br />
      table.f:=ilc301<br />
      table.t:=ilc400<br />
<br />
      table.f:=ilc401<br />
      table.t:=ilc401<br />
<br />
      table.f:=ilc402<br />
      table.t:=ilc999<br />
<br />
      table.f:=inv000<br />
      table.t:=inv699<br />
<br />
      table.f:=inv700<br />
      table.t:=inv799<br />
<br />
      table.f:=inv800<br />
      table.t:=inv999<br />
<br />
      table.f:=inw<br />
      table.t:=zzz999<br />
<br />
You may see non-existent module IDs (example, inw) - that is to make sure that a new table is not missed out in case of any customizations.<br />
<br />
The job is set up to download a range, then compress the download, one range at a time. The jobs are setup to fire after 30 minute intervals. The hope is always that the system will not get choked up, so this activity is usually done after midnight of a Saturday to get a full day for the download. <br />
<br />
When setting up the job, you can specify the error log files, which help to find if there was any failure.<br />
<br />
Hope this helps.</div></div><hr />


<div class="post"><div class="posttop"><div class="username">Ajesh</div><div class="date">12th August 2013, 16:45</div></div><div class="posttext">One of the btter option is to use a Text File containing the list of Tables and input it as a Parameter to the bdbpre command. It works<br />
<br />
<br />
Something on the lines of <br />
<br />
/baan/app/bse/bin/bdbpre6.1  -pB40Uc4 -t&quot;^a&quot; -o&quot;/temp/toolsdump516&quot; -I&quot;/temp/tablist<br />
<br />
<br />
You may or may not put Package Combination in the paremeters</div></div><hr />



</div>
</body>
</html>