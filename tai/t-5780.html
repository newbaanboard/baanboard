<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="keywords" content="bdbpost and dump files &gt; 2GB, baan,baanerp,erp,forum,discussion,bulletin board" />
	<meta name="description" content="[Archive] bdbpost and dump files &gt; 2GB Tools Administration &amp; Installation" />
	
	<title>bdbpost and dump files &gt; 2GB [Archive]  - Baanboard.com</title>
	<link rel="stylesheet" type="text/css" href="../styles.css" />
</head>
<body>
<div class="pagebody">
<div id="navbar"><a href="../index.html">Newbaanboard</a> &gt; <a href="../index.html">Baan Quick Support: Functional &amp; Technical</a> &gt; <a href="../index.html">Tools Administration &amp; Installation</a> &gt; bdbpost and dump files &gt; 2GB</div>
<hr />
<div class="pda"></div>

<hr />

<div class="post"><div class="posttop"><div class="username">sgabor</div><div class="date">10th July 2002, 16:11</div></div><div class="posttext">I did a sequential dump of one of our production companies and for the first time ended up with a dump file &gt; 2GB.  In fact, I ended up with 3 files:<br />
<br />
CO160.dmp, size 2,097,152KB - The file specified in the bdbpre script<br />
CO160.dmp.1 size 2,097,152KB<br />
CO160.dmp.2 size 133,472KB<br />
<br />
If I try to load the data using: <br />
<br />
D:\Baan\bin\bdbpost -kmnif -I L:\CO160.DMP -q L:\CO160LOAD.ERR<br />
<br />
it only loads the data actually in DO160.dmp.  If I try and load the data using:<br />
<br />
D:\Baan\bin\bdbpost -kmnif -I L:\CO160.DMP.1 -q L:\CO160LOAD.ERR<br />
<br />
I get the message:<br />
<br />
Default separator ('\0') taken<br />
Reading dump header failed<br />
BDBPOST: FATAL ERROR: 'Not a valid dump file'<br />
<br />
I presume I am missing a flag or something.  Any help or suggestions would be appreciated.<br />
<br />
Thanks<br />
<br />
Stephen</div></div><hr />


<div class="post"><div class="posttop"><div class="username">patvdv</div><div class="date">10th July 2002, 16:15</div></div><div class="posttext">Stephen,<br />
<br />
How did you split the files in 3 parts. I think there lies the problem, I can't see how many Baan's bdbpost would be able to 'pick up' where the previous file ended. <br />
<br />
Why don't you use a table based export? In that way you will get a separate file for each  table and you will have less chance of running into the 2GB limit.</div></div><hr />


<div class="post"><div class="posttop"><div class="username">sgabor</div><div class="date">10th July 2002, 16:26</div></div><div class="posttext">The problem is BDBPRE made the &quot;decision&quot; to create the .1 and .2 files (I did not split them).  I started the dump Sunday morning.  When I got into the office Monday morning.  I had the three files.  Since this is the first time I have had a dump file exceed 2GB, I'm not really certain what or how.  <br />
<br />
To your point, I understand how, and why, to create two different dump files based on two or more different &quot;table lists&quot;.  But that doesn't solve my immediate problem.  Short of doing another dump, how do I get BDBPOST to load the .1 and .2 files that BDBPRE created.<br />
<br />
Thanks<br />
<br />
Stephen</div></div><hr />


<div class="post"><div class="posttop"><div class="username">Djie-En</div><div class="date">10th July 2002, 16:39</div></div><div class="posttext">Hi,<br />
<br />
Try to use the parameter -M (capital 'm') also into your command.<br />
(It is possible by the bdbpre command to use -M and then followed by a number (i.e. -M 500M) which will give a file of about 500 Megabites (or 500K = a file of 500 kilobites). <br />
<br />
GN</div></div><hr />


<div class="post"><div class="posttop"><div class="username">patvdv</div><div class="date">10th July 2002, 16:44</div></div><div class="posttext">Stephen,<br />
<br />
This would be new functionality of bdbpre and bdbpost  to me. Does your first  file have a 'header' section? I know Baan came up with a new dump format a while ago and possibly introduced the split-feature then as well.<br />
<br />
If you were on UNIX you could use a named pipe to pass the files through one by one but on NT....</div></div><hr />


<div class="post"><div class="posttop"><div class="username">Djie-En</div><div class="date">10th July 2002, 16:50</div></div><div class="posttext">Hi,<br />
<br />
If you do a bdbpre with the -M 500M flag it will creates files of about 500 Megabites. (So not only one file, if you have more data in your company or table))<br />
If i do a dump from our tfgld106 it then creates about 10 files of 500 MB.<br />
As Patvdv already proposed, it is better to do it by table, because if there is a problem with bdbpost, you are able to start with the table which gave an error and it is not necessary to do a bdbpost for the other tables. <br />
<br />
GN</div></div><hr />


<div class="post"><div class="posttop"><div class="username">sgabor</div><div class="date">10th July 2002, 17:00</div></div><div class="posttext">So, if you use the -M flag how does bdbpost know to link all the files back together?  Is there a flag you need to use on bdbpost?  If so, would it work in my situation?</div></div><hr />


<div class="post"><div class="posttop"><div class="username">norwim</div><div class="date">10th July 2002, 18:02</div></div><div class="posttext">Hi sgabor!<br />
<br />
I suppose that you didn't run bdbpre for quite a while - to me it looks as if your dump totals &gt; 4GB<br />
what happens if you simply say &quot;bdbpost6.1 .... &lt; CO160.dmp*&quot;?<br />
By the naming convention, the first file should be used first (the one with the header and no filename suffix), then the one with suffix .1, then .2 ?<br />
Please try and give a feedback<br />
<br />
good luck<br />
<br />
Norbert</div></div><hr />


<div class="post"><div class="posttop"><div class="username">Djie-En</div><div class="date">10th July 2002, 18:07</div></div><div class="posttext">Hi,<br />
<br />
According to me is the -M flag a signal in the program, which recognize that there should be more dmp-files then only 1(one).<br />
I use th bdbpost en bdbpre commands on UNIX in a script, because of the 2.14GB limit. (I use it at this moment for 3 TF tables)<br />
But, according to me, if you did the dump by the BAAN-session, it should be that you also can import the dump by the BAAN-session. Isn't it? <br />
<br />
GN</div></div><hr />


<div class="post"><div class="posttop"><div class="username">Ruskin</div><div class="date">10th July 2002, 23:20</div></div><div class="posttext">Check out Baan case number 107090. <br />
<br />
Basically, the file that has been created, should (ideally) not exceed 2Gb. The solution (listed in case number 107090) details how to create multiple files from the same table. NOTE: this is for porting sets 6.1c.05 and up.<br />
<br />
To create the files, use a syntax similar to;<br />
bdbpre6.1 -M1G -O/home/bsp/g_l -Ntfgld410 -C100<br />
This will create multiple dump files (from the tfgld410 table of company 100, each dump file being no greater than 1Gb), named;<br />
g_l<br />
g_l.1<br />
g_l.2<br />
....<br />
<br />
To import the files into a different company, use a syntax similar to;<br />
bdbpost6.1 -A -M -c900 -I/home/bsp/g_l<br />
where the company you are importing into, is company 900.<br />
<br />
Please refer to solution # 107090 for a more detail description.</div></div><hr />


<div class="post"><div class="posttop"><div class="username">Nicholas</div><div class="date">13th July 2002, 05:08</div></div><div class="posttext">I usually do:<br />
bdbpre6.1 -I tfgld106 -t&quot;|&quot; -M 1024M -o tfgld106_out -q <br />
<br />
then <br />
<br />
bdbpost6.1 -n -m -k -M -D ./tfgld106_out -t &quot;|&quot; -q log_tfgld106<br />
<br />
The output you would get...<br />
tfgld106_out.S<br />
tfgld106_out.1<br />
tfgld106_out.2<br />
tfgld106_out.3<br />
tfgld106_out.4<br />
etc<br />
<br />
Based on the size of the table, you can adjust the -M option to the size you would like.<br />
<br />
The first file that is outputted (.S) will have 3 rows of header information.   The rest of the files will not.<br />
<br />
If for some reason the bdbpost fails, and it has already loaded the first file, you can copy tfgld106_out.1 to tfgld106_out.S, and continue your data load.<br />
<br />
If you are back porting the pre files, you can strip out the 3 lines of header information to load into an older porting set.<br />
<br />
Hope that helps...</div></div><hr />


<div class="post"><div class="posttop"><div class="username">wei_smooth</div><div class="date">19th January 2006, 04:50</div></div><div class="posttext">hallo,<br />
<br />
<br />
I'm having the same problem.<br />
I creating dump with ttaad4226m000.<br />
when the data is exceded 2 Gb it will create another file.<br />
example: com.777 and com.777.1<br />
<br />
right now I've try using tdaad4227m000 to create table from dump.<br />
but there only one file is being used while there is 2 dump file.<br />
<br />
how can I import the second file<br />
<br />
please advice<br />
<br />
<br />
thanks</div></div><hr />


<div class="post"><div class="posttop"><div class="username">Han Brinkman</div><div class="date">19th January 2006, 13:49</div></div><div class="posttext">correct me if i am wrong but i guess you have to do this on a commandline since the -M parameter can be turn on in the session.<br />
<br />
Rgrds,<br />
Han</div></div><hr />


<div class="post"><div class="posttop"><div class="username">dave_23</div><div class="date">19th January 2006, 13:57</div></div><div class="posttext">I think there is a patch to tdaad4227m000 that fixes that.<br />
<br />
Dave</div></div><hr />


<div class="post"><div class="posttop"><div class="username">Han Brinkman</div><div class="date">19th January 2006, 17:43</div></div><div class="posttext">I am on BaanV sp17 but that doesn't show an option. Is it hidden? Or does this program check your os? (I have to live with windows).</div></div><hr />


<div class="post"><div class="posttop"><div class="username">i96nds</div><div class="date">19th January 2006, 19:19</div></div><div class="posttext">Hi, long time... no see...<br />
<br />
Han Brinkman, I know that ttaad4226m000 by default &quot;calls&quot; bdbpre6.1 with the option &quot;-M 2G&quot;. <br />
I even have the impression that also in the command line the bdbpre6.1 has a default setting &quot;-M 2G&quot;. <br />
<br />
So, for wei_smooth, the files were split due to these default settings.<br />
<br />
Basically, BAAN should be able by itself to import from all files it has generated. But there are also bugs, as dave_23 has mentioned. <br />
<br />
So you have to patch your SW and install latest version for session ttaad4226m000 and ttaad4227m000 (in fact latest scripts) <br />
<br />
But since these sessions use bdbpre/bdbpost you must also upgrade to the latest porting set. <br />
<br />
P.S. I remember that there were bugs in PS 6.1c.05.02 when you explicitly mentioned &quot;-M some_size&quot;. If you did that, bdbpost was not able to import anymore all generated export files.</div></div><hr />


<div class="post"><div class="posttop"><div class="username">ghabbour</div><div class="date">10th March 2006, 06:55</div></div><div class="posttext">Hi <br />
I did the following<br />
1- use bdbpre6.1 to dump a table making the file size no more than 200k<br />
the table was 370k thus I have 2 files for the same table<br />
bdbpre6.1 -Ntfgld410 -t'|'  -M200K -C444<br />
<br />
2- put the 2 files in the same directory and used bdbpost6.1<br />
<br />
 bdbpost6.1 -t'|' -R -D/dump/san/bk380/test -C444<br />
<br />
it worked just fine, try it with your files may be it will work</div></div><hr />



</div>
</body>
</html>