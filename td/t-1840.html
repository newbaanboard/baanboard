<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<meta name="keywords" content="Commit, baan,baanerp,erp,forum,discussion,bulletin board" />
	<meta name="description" content="[Archive] Commit Tools Development" />
	
	<title>Commit [Archive]  - Baanboard.com</title>
	<link rel="stylesheet" type="text/css" href="../styles.css" />
</head>
<body>
<div class="pagebody">
<div id="navbar"><a href="../index.html">Newbaanboard</a> &gt; <a href="../index.html">Baan Quick Support: Functional &amp; Technical</a> &gt; <a href="../index.html">Tools Development</a> &gt; Commit</div>
<hr />
<div class="pda"></div>

<hr />

<div class="post"><div class="posttop"><div class="username">srinivas</div><div class="date">26th April 2002, 12:37</div></div><div class="posttext">When baan generates update session by default the script commits after 50 transactions. What are the benefits of this approach?<br />
<br />
Suppose I have a table where 30000 records are there. I change one field in all the records and do a single commit. What difference in performance this approach would have compared to baan standard update session where it commits after every few transactions?<br />
<br />
Regards,<br />
Srinivas</div></div><hr />


<div class="post"><div class="posttop"><div class="username">Youp2001</div><div class="date">26th April 2002, 14:00</div></div><div class="posttext">I've heart once that when too many records are updated within a single transaction at some point Baan can't handle all these changes internally and will temporarily write the changes to a swapfile on disk. This is time consuming. Besides that, Baan will lock all changed records in this situaton (so delayed locks are changed into hard locks) which means that other users can get locking errors when accessing same data.<br />
<br />
Youp</div></div><hr />


<div class="post"><div class="posttop"><div class="username">agramm</div><div class="date">26th April 2002, 14:19</div></div><div class="posttext">It is not only topic of performance.<br />
In worst case commit will never be done, because such a lot of records cannot be buffered.<br />
Normally commit will be used with db.retry.point. If commit fails baan will go back do retry point and update records again and try to commit.<br />
Commit can fail without fatal errors in database so you would not notity.<br />
<br />
Greetings Andy</div></div><hr />


<div class="post"><div class="posttop"><div class="username">MariaC</div><div class="date">26th April 2002, 14:51</div></div><div class="posttext">One benefit of a single commit that I can think of is if you are updating a group of records based on similar criteria.  If one of these updates fail then the whole transaction will roll back and no records will be updated.  If you do a commit after each update there is no way to roll back the transaction.<br />
<br />
However with your example of 30000 records you will need to make sure that you have sufficient system resources available to cater for the huge transaction that you will be creating.</div></div><hr />



</div>
</body>
</html>